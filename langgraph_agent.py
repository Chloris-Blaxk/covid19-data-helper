import os
from typing import TypedDict, Annotated, Sequence
import operator
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from tools import all_tools

# Load environment variables from .env file
load_dotenv()

# Get API key and optional base URL from environment variables
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_API_BASE")

if not api_key:
    raise ValueError("Please set the OPENAI_API_KEY environment variable in a .env file.")

# 1. Define the agent state
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]

# 2. Define the nodes

# The agent node
def call_model(state):
    messages = state['messages']
    # Pass the API key and base_url to the client
    model = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0, 
        streaming=True, 
        api_key=api_key,
        base_url=base_url
    )
    # Bind tools to the model
    model_with_tools = model.bind_tools(all_tools)
    response = model_with_tools.invoke(messages)
    return {"messages": [response]}

# The tool node using the new ToolNode class
tool_node = ToolNode(all_tools)

# 3. Define the edges
def should_continue(state):
    last_message = state['messages'][-1]
    # If there are no tool calls, we're done
    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
        return "end"
    # Otherwise, continue with tool execution
    else:
        return "continue"

# 4. Assemble the graph
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node) # Use the ToolNode instance directly
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", "agent")

# Compile the graph into a runnable app
app = workflow.compile()

def run_agent(query: str):
    """
    Runs the LangGraph agent with the given query and returns the final content.
    The content is expected to be a Markdown string generated by the LLM.
    """
    inputs = {"messages": [HumanMessage(content=query)]}
    final_state = app.invoke(inputs)
    
    # The final response is the last message from the agent, which is the complete report.
    return final_state['messages'][-1].content

if __name__ == '__main__':
    # Example usage
    query1 = "Generate a plot of the top 5 countries by confirmed cases."
    result1 = run_agent(query1)
    print(f"Query: {query1}\nResult: {result1}\n")

    query2 = "What is the mortality rate in Italy?"
    result2 = run_agent(query2)
    print(f"Query: {query2}\nResult: {result2}\n")
